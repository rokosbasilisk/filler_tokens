{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loki/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from data_utils import *\n",
    "from nethook import *\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from utils import InputEmbedCausalTransformer\n",
    "import json\n",
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "import colorama\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorama.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/home/loki/projects/filler_tokens/checkpoints/2024-07-21-17-minidata-checkpoint-final/model_weights.pt\"\n",
    "CONFIG_FILE = \"/home/loki/projects/filler_tokens/configs/llama_d384l4h6.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/minidata_trainset_2024-07-21.csv', header=None, names=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate encodings\n",
      "raw input 0  205 066 491 653 677 358 153 311 875 014 P 0- 6 2- 6 0- 8 4- 8 0- 3 6- 3 7- 1 0- 7 9- 1 2- 3- 3- 6 1- 3 1- 1 6- 9 1- 3 8- 3 9- 0 2- 0 2- 8 5- 7 2- 4 2- 7 8- 6 2- 0 3- 0 3- 1 3- 7 7- 6 8- 8 9- 7 5- 9 6- 2 7- 9 8- 4 9- 6 5- 4 7- 6 8- 2 5- 2 7- 4 6- 8 9- 6 8- 1 9- 5 8- 8 9- 8 A True\n",
      "encoded sample 0 {'input_ids': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    5,   31,\n",
      "           7,   31,    5,   33,    9,   33,    5,   28,   11,   28,   12,   26,\n",
      "           5,   32,   14,   26,    7,    8,    8,   31,    6,   28,    6,   26,\n",
      "          11,   34,    6,   28,   13,   28,   14,   25,    7,   25,    7,   33,\n",
      "          10,   32,    7,   29,    7,   32,   13,   31,    7,   25,    8,   25,\n",
      "           8,   26,    8,   32,   12,   31,   13,   33,   14,   32,   10,   34,\n",
      "          11,   27,   12,   34,   13,   29,   14,   31,   10,   29,   12,   31,\n",
      "          13,   27,   10,   27,   12,   29,   11,   33,   14,   31,   13,   26,\n",
      "          14,   30,   13,   33,   14,   33,    4,    1,    0, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100])}\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "train_set = Match3VectorDataset(train_df, 3, 10, 10, 'P')\n",
    "print(train_set.input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockOutputWrapper(torch.nn.Module):\n",
    "    def __init__(self, block, unembed_matrix, norm):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.unembed_matrix = unembed_matrix\n",
    "        self.norm = norm\n",
    "        self.block_output_unembedded = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.block(*args, **kwargs)\n",
    "        if isinstance(output, tuple):\n",
    "            self.block_output_unembedded = self.unembed_matrix(self.norm(output[0]))\n",
    "            return output\n",
    "        else:\n",
    "            self.block_output_unembedded = self.unembed_matrix(self.norm(output))\n",
    "            return output\n",
    "\n",
    "    def reset_block_output(self):\n",
    "        self.block_output_unembedded = None\n",
    "\n",
    "class LlamaHelper:\n",
    "    def __init__(self, config_file, model_path, train_set):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        config = AutoConfig.from_pretrained(config_file)\n",
    "        model = InputEmbedCausalTransformer(AutoModelForCausalLM.from_config(config), train_set.input_dim)\n",
    "        state_dict = torch.load(model_path)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        model = model.to(self.device)\n",
    "        self.model = model\n",
    "        self.word_index_map = train_set.word_index_map\n",
    "        self.data_len = train_set.data_len\n",
    "        self.mod = train_set.mod\n",
    "        \n",
    "        for i, layer in enumerate(self.model.base_model.model.layers):\n",
    "            self.model.base_model.model.layers[i] = BlockOutputWrapper(layer, self.model.base_model.lm_head, self.model.base_model.model.norm)\n",
    "\n",
    "    def decode_tensors(self, sequence):\n",
    "        decoded_sequence = []\n",
    "        marker_found = False\n",
    "\n",
    "        for token in sequence:\n",
    "            token = token.item()\n",
    "            if token == -100:\n",
    "                if not marker_found:\n",
    "                    decoded_sequence.append(\"[MASK]\")\n",
    "                continue\n",
    "            elif token == 0:\n",
    "                decoded_sequence.append(\"[EOS]\")\n",
    "                break  # Stop decoding after EOS\n",
    "            elif token < len(self.word_index_map):\n",
    "                # Regular word\n",
    "                word = list(self.word_index_map.keys())[list(self.word_index_map.values()).index(token)]\n",
    "                decoded_sequence.append(word)\n",
    "                if word in [\"A\", \"P\"]:\n",
    "                    marker_found = True\n",
    "            else:\n",
    "                # Handle digit labels\n",
    "                offset = len(self.word_index_map)\n",
    "                if token < offset + self.data_len * 2:\n",
    "                    # Tuple index encoding\n",
    "                    idx = (token - offset) % self.data_len\n",
    "                    tuple_pos = (token - offset) // self.data_len\n",
    "                    decoded_sequence.append(f\"{tuple_pos}-{idx}\")\n",
    "                else:\n",
    "                    # Single digit or digit in tuple\n",
    "                    char_pos = (token - offset - self.data_len * 2) // self.mod\n",
    "                    digit = (token - offset - self.data_len * 2) % self.mod\n",
    "                    if char_pos == 0 or len(decoded_sequence) == 0 or not decoded_sequence[-1][-1].isdigit():\n",
    "                        decoded_sequence.append(str(digit))\n",
    "                    else:\n",
    "                        decoded_sequence[-1] += str(digit)\n",
    "\n",
    "        return \" \".join(decoded_sequence)\n",
    "\n",
    "    def generate_text(self, inputs, max_length=100):\n",
    "        generate_ids = self.model.generate(inputs['input_ids'].float().to(self.device), max_length=max_length)\n",
    "        return self.decode_tensors(generate_ids)[0]\n",
    "\n",
    "    def get_logits(self, inputs):\n",
    "        with torch.no_grad():\n",
    "            input_ids = inputs['input_ids'].float().to(self.device).unsqueeze(0)\n",
    "            out = self.model(input_ids)\n",
    "            return out.logits\n",
    "\n",
    "    def set_add_attn_output(self, layer, add_output):\n",
    "        self.model.base_model.model.layers[layer].attn_add_tensor(add_output)\n",
    "\n",
    "    def get_attn_activations(self, layer):\n",
    "        return self.model.base_model.model.layers[layer].get_attn_activations()\n",
    "    \n",
    "\n",
    "    def reset_all_layers(self):\n",
    "        for layer in self.model.base_model.model.layers:\n",
    "            layer.reset_block_output()\n",
    "\n",
    "    def decode_all_layers(self, inputs, topk=10, print_prob=False):\n",
    "        self.reset_all_layers()\n",
    "        logits = self.get_logits(inputs)\n",
    "\n",
    "        for i, layer in enumerate(self.model.base_model.model.layers):\n",
    "            print(f'\\nLayer {i}: Decoded intermediate outputs')\n",
    "            if layer.block_output_unembedded is not None:\n",
    "                try:\n",
    "                    self.print_decoded_activations_horizontal(layer.block_output_unembedded, 'Block output', topk=topk, print_prob=print_prob)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing layer {i}: {str(e)}\")\n",
    "                    print(f\"Shape of block_output_unembedded: {layer.block_output_unembedded.shape}\")\n",
    "            else:\n",
    "                print(\"No intermediate output available for this layer.\")\n",
    "\n",
    "        # Print final logits\n",
    "        print(\"\\nFinal output logits:\")\n",
    "        try:\n",
    "            self.print_decoded_activations_horizontal(logits, 'Final output', topk=topk, print_prob=print_prob)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing final logits: {str(e)}\")\n",
    "            print(f\"Shape of logits: {logits.shape}\")\n",
    "\n",
    "    def print_decoded_activations_horizontal(self, decoded_activations, label, topk=10, print_prob=False):\n",
    "        seq_length = decoded_activations.size(1)\n",
    "        all_tokens = []\n",
    "        all_probs = []\n",
    "\n",
    "        for i in range(seq_length):\n",
    "            softmaxed = torch.nn.functional.softmax(decoded_activations[0][i], dim=-1)\n",
    "            values, indices = torch.topk(softmaxed, min(topk, len(softmaxed)))\n",
    "            probs_percent = [int(v * 100) for v in values.tolist()]\n",
    "            tokens = self.decode_tensors(indices.unsqueeze(-1)).split()\n",
    "            all_tokens.append(tokens)\n",
    "            all_probs.append(probs_percent)\n",
    "\n",
    "        # Find the maximum number of predictions available for any token\n",
    "        max_predictions = max(len(tokens) for tokens in all_tokens)\n",
    "\n",
    "        # Print tokens horizontally\n",
    "        for k in range(min(topk, max_predictions)):\n",
    "            token_row = []\n",
    "            for i, tokens in enumerate(all_tokens):\n",
    "                if k < len(tokens):\n",
    "                    token_row.append(f\"{i}:{tokens[k]:<8}\")\n",
    "                else:\n",
    "                    token_row.append(f\"{i}:{'---':<8}\")\n",
    "            print(f\"Top {k+1}: \" + \" \".join(token_row))\n",
    "\n",
    "        # Print probabilities if requested\n",
    "        if print_prob:\n",
    "            print(\"\\nProbabilities:\")\n",
    "            for k in range(min(topk, max_predictions)):\n",
    "                prob_row = []\n",
    "                for i, probs in enumerate(all_probs):\n",
    "                    if k < len(probs):\n",
    "                        prob_row.append(f\"{i}:{probs[k]:3d}%\")\n",
    "                    else:\n",
    "                        prob_row.append(f\"{i}:{'---':>3}\")\n",
    "                print(f\"Top {k+1}: \" + \" \".join(prob_row))\n",
    "\n",
    "        print()  # Add a blank line for readability\n",
    "\n",
    "    def print_top_logit_progression(self, inputs, n_highest=1, chunk_size=10):\n",
    "        self.reset_all_layers()\n",
    "        logits = self.get_logits(inputs)\n",
    "        \n",
    "        num_layers = len(self.model.base_model.model.layers)\n",
    "        seq_length = inputs['input_ids'].size(1)\n",
    "\n",
    "        # Initialize matrices to store nth highest tokens and their logits\n",
    "        nth_tokens = np.empty((num_layers + 1, seq_length), dtype=object)\n",
    "        nth_logits = np.zeros((num_layers + 1, seq_length))\n",
    "        \n",
    "        # Process each layer\n",
    "        for i in range(num_layers):\n",
    "            layer = self.model.base_model.model.layers[i]\n",
    "            if layer.block_output_unembedded is not None:\n",
    "                for j in range(min(layer.block_output_unembedded.size(1), seq_length)):\n",
    "                    values, indices = torch.topk(layer.block_output_unembedded[0][j], n_highest)\n",
    "                    nth_tokens[i][j] = self.decode_tensors(indices[-1].unsqueeze(-1)).strip()\n",
    "                    nth_logits[i][j] = values[-1].item()\n",
    "        \n",
    "        # Process final output\n",
    "        for j in range(min(logits.size(1), seq_length)):\n",
    "            values, indices = torch.topk(logits[0][j], n_highest)\n",
    "            nth_tokens[-1][j] = self.decode_tensors(indices[-1].unsqueeze(-1)).strip()\n",
    "            nth_logits[-1][j] = values[-1].item()\n",
    "        \n",
    "        # Print the progression in chunks\n",
    "        print(f\"{n_highest}th highest logit progression:\")\n",
    "        \n",
    "        for chunk_start in range(0, seq_length, chunk_size):\n",
    "            chunk_end = min(chunk_start + chunk_size, seq_length)\n",
    "            print(f\"\\nTokens {chunk_start} to {chunk_end - 1}:\")\n",
    "            print(f\"Layer | \" + \" | \".join([f\"Token {i}\" for i in range(chunk_start, chunk_end)]))\n",
    "            print(\"-\" * (7 + 8 * (chunk_end - chunk_start)))\n",
    "            \n",
    "            for i in range(num_layers + 1):\n",
    "                layer_name = f\"h{i}_out\" if i < num_layers else \"h_out\"\n",
    "                tokens = [f\"{token:<6}\" for token in nth_tokens[i][chunk_start:chunk_end]]\n",
    "                print(f\"{layer_name:<5} | \" + \" | \".join(tokens))\n",
    "            \n",
    "\n",
    "    @staticmethod\n",
    "    def get_color_code(value):\n",
    "        # Convert value to a color (you can adjust this to match your preferred color scheme)\n",
    "        r = int(255 * (1 - value))\n",
    "        b = int(255 * value)\n",
    "        return f'\\033[38;2;{r};0;{b}m'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaHelper(CONFIG_FILE, MODEL_PATH, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th highest logit progression:\n",
      "\n",
      "Tokens 0 to 9:\n",
      "Layer | Token 0 | Token 1 | Token 2 | Token 3 | Token 4 | Token 5 | Token 6 | Token 7 | Token 8 | Token 9\n",
      "---------------------------------------------------------------------------------------\n",
      "h0_out | 0-1    | 0-8    | 6      | 0-0    | .      | .      | 0-0    | 0-0    | 0-0    | 0-0   \n",
      "h1_out | .      | .      | .      | .      | .      | .      | .      | .      | .      | .     \n",
      "h2_out | .      | .      | .      | .      | .      | .      | .      | .      | .      | .     \n",
      "h3_out | .      | .      | .      | .      | .      | .      | .      | .      | .      | .     \n",
      "h_out | .      | .      | .      | .      | .      | .      | .      | .      | .      | .     \n",
      "\n",
      "Tokens 10 to 19:\n",
      "Layer | Token 10 | Token 11 | Token 12 | Token 13 | Token 14 | Token 15 | Token 16 | Token 17 | Token 18 | Token 19\n",
      "---------------------------------------------------------------------------------------\n",
      "h0_out | .      | 4      | 0-2    | 4      | 0-2    | 4      | 0-2    | 4      | 0-2    | 4     \n",
      "h1_out | 0-1    | 0      | 0-2    | 4      | 0-2    | 4      | 0-2    | 4      | 0-8    | 4     \n",
      "h2_out | .      | 3      | 0-0    | 3      | 0-0    | 4      | 0-0    | 4      | 0-0    | 3     \n",
      "h3_out | .      | 2      | 0-2    | 1      | 0-0    | 0      | 0-4    | 9      | 0-5    | 8     \n",
      "h_out | .      | 2      | 0-2    | 1      | 0-0    | 0      | 0-4    | 9      | 0-5    | 8     \n",
      "\n",
      "Tokens 20 to 29:\n",
      "Layer | Token 20 | Token 21 | Token 22 | Token 23 | Token 24 | Token 25 | Token 26 | Token 27 | Token 28 | Token 29\n",
      "---------------------------------------------------------------------------------------\n",
      "h0_out | 0-6    | 0      | 0-2    | 4      | 0-2    | 0      | 0-9    | 4      | 0-2    | 4     \n",
      "h1_out | 0-6    | 4      | 0-8    | 4      | 0-8    | 4      | 0-9    | 4      | 0-2    | 4     \n",
      "h2_out | 0-0    | 4      | 0-0    | 4      | 0-0    | 3      | 0-0    | 4      | 0-1    | 4     \n",
      "h3_out | 0-0    | 1      | 0-7    | 0      | 0-0    | 7      | 0-9    | 4      | 0-1    | 9     \n",
      "h_out | 0-0    | 1      | 0-7    | 0      | 0-0    | 7      | 0-9    | 4      | 0-1    | 9     \n",
      "\n",
      "Tokens 30 to 39:\n",
      "Layer | Token 30 | Token 31 | Token 32 | Token 33 | Token 34 | Token 35 | Token 36 | Token 37 | Token 38 | Token 39\n",
      "---------------------------------------------------------------------------------------\n",
      "h0_out | 0-4    | 4      | 0-4    | 4      | 0-4    | 4      | 0-6    | 4      | 0-7    | 0     \n",
      "h1_out | 0-3    | 4      | 0-4    | 4      | 0-1    | 4      | 0-1    | 4      | 0-7    | 3     \n",
      "h2_out | 0-1    | 3      | 0-1    | 4      | 0-1    | 4      | 0-1    | 3      | 0-7    | 3     \n",
      "h3_out | 0-3    | 8      | 0-1    | 1      | 0-1    | 8      | 0-1    | 8      | 0-1    | 8     \n",
      "h_out | 0-3    | 8      | 0-1    | 1      | 0-1    | 8      | 0-1    | 8      | 0-1    | 8     \n",
      "\n",
      "Tokens 40 to 49:\n",
      "Layer | Token 40 | Token 41 | Token 42 | Token 43 | Token 44 | Token 45 | Token 46 | Token 47 | Token 48 | Token 49\n",
      "---------------------------------------------------------------------------------------\n",
      "h0_out | 0-8    | 4      | 0-9    | 4      | 0-2    | 4      | 0-4    | 4      | 0-4    | 4     \n",
      "h1_out | 0-8    | 4      | 0-9    | 4      | 0-9    | 4      | 0-4    | 4      | 0-9    | 4     \n",
      "h2_out | 0-8    | 4      | 0-9    | 4      | 0-3    | 4      | 0-4    | 4      | 0-4    | 4     \n",
      "h3_out | 0-1    | 5      | 0-1    | 1      | 0-2    | 4      | 0-4    | 4      | 0-2    | 8     \n",
      "h_out | 0-1    | 5      | 0-1    | 1      | 0-2    | 4      | 0-4    | 4      | 0-2    | 8     \n",
      "\n",
      "Tokens 50 to 59:\n",
      "Layer | Token 50 | Token 51 | Token 52 | Token 53 | Token 54 | Token 55 | Token 56 | Token 57 | Token 58 | Token 59\n",
      "---------------------------------------------------------------------------------------\n",
      "h0_out | 0-7    | 4      | 0-7    | 0      | 0-8    | 4      | 0-9    | 0      | 0-9    | 4     \n",
      "h1_out | 0-9    | 4      | 0-7    | 4      | 0-8    | 4      | 0-9    | 4      | 0-9    | 4     \n",
      "h2_out | 0-6    | 3      | 0-7    | 3      | 0-8    | 4      | 0-9    | 4      | 0-3    | 4     \n",
      "h3_out | 0-2    | 0      | 0-7    | 4      | 0-2    | 4      | 0-2    | 0      | 0-4    | 9     \n",
      "h_out | 0-2    | 0      | 0-7    | 4      | 0-2    | 4      | 0-2    | 0      | 0-4    | 9     \n",
      "\n",
      "Tokens 60 to 64:\n",
      "Layer | Token 60 | Token 61 | Token 62 | Token 63 | Token 64\n",
      "-----------------------------------------------\n",
      "h0_out | 0-4    | 4      | 0-4    | 4      | 0-7   \n",
      "h1_out | 0-4    | 4      | 0-4    | 4      | 0-7   \n",
      "h2_out | 0-4    | 3      | 0-6    | 3      | 0-7   \n",
      "h3_out | 0-5    | 9      | 0-3    | 9      | 0-7   \n",
      "h_out | 0-5    | 9      | 0-3    | 9      | 0-7   \n"
     ]
    }
   ],
   "source": [
    "model.print_top_logit_progression(train_set[50],1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2th highest logit progression:\n",
      "\n",
      "Tokens 0 to 9:\n",
      "Layer | Token 0 | Token 1 | Token 2 | Token 3 | Token 4 | Token 5 | Token 6 | Token 7 | Token 8 | Token 9\n",
      "---------------------------------------------------------------------------------------\n",
      "h0_out | 0-0    | 0-1    | 0-0    | 0-4    | 0-0    | 0-0    | 0      | 6      | .      | .     \n",
      "h1_out | 0-1    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0   \n",
      "h2_out | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0   \n",
      "h3_out | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0   \n",
      "h_out | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0    | 0-0   \n",
      "\n",
      "Tokens 10 to 19:\n",
      "Layer | Token 10 | Token 11 | Token 12 | Token 13 | Token 14 | Token 15 | Token 16 | Token 17 | Token 18 | Token 19\n",
      "---------------------------------------------------------------------------------------\n",
      "h0_out | 0-1    | 0      | 0-8    | 0      | 0-8    | 0      | 0-8    | 0      | 0-5    | 0     \n",
      "h1_out | .      | 4      | 0-8    | 0      | 0-3    | 0      | 0-8    | 1      | 0-5    | 0     \n",
      "h2_out | 0-1    | 1      | 0-2    | 4      | 0-3    | 3      | 0-3    | 3      | 0-5    | 1     \n",
      "h3_out | 0-1    | 3      | 0-0    | 3      | 0-3    | 1      | 0-0    | 0      | 0-0    | 1     \n",
      "h_out | 0-1    | 3      | 0-0    | 3      | 0-3    | 1      | 0-0    | 0      | 0-0    | 1     \n",
      "\n",
      "Tokens 20 to 29:\n",
      "Layer | Token 20 | Token 21 | Token 22 | Token 23 | Token 24 | Token 25 | Token 26 | Token 27 | Token 28 | Token 29\n",
      "---------------------------------------------------------------------------------------\n",
      "h0_out | 0-7    | 4      | 0-8    | 0      | 0-8    | 4      | 0-7    | 0      | 0-8    | 0     \n",
      "h1_out | 0-8    | 0      | 0-7    | 0      | 0-2    | 0      | 0-2    | 3      | 0-3    | 0     \n",
      "h2_out | 0-6    | 3      | 0-7    | 3      | 0-8    | 0      | 0-9    | 3      | 0-2    | 3     \n",
      "h3_out | 0-6    | 4      | 0-0    | 1      | 0-8    | 5      | 0-0    | 3      | 0-2    | 3     \n",
      "h_out | 0-6    | 4      | 0-0    | 1      | 0-8    | 5      | 0-0    | 3      | 0-2    | 3     \n",
      "\n",
      "Tokens 30 to 39:\n",
      "Layer | Token 30 | Token 31 | Token 32 | Token 33 | Token 34 | Token 35 | Token 36 | Token 37 | Token 38 | Token 39\n",
      "---------------------------------------------------------------------------------------\n",
      "h0_out | 0-2    | 1      | 0-5    | 1      | 0-2    | 0      | 0-8    | 2      | 0-8    | 2     \n",
      "h1_out | 0-1    | 0      | 0-1    | 1      | 0-8    | 0      | 0-6    | 3      | 0-1    | 4     \n",
      "h2_out | 0-3    | 4      | 0-3    | 3      | 0-5    | 3      | 0-6    | 4      | 0-1    | 4     \n",
      "h3_out | 0-1    | 5      | 0-4    | 2      | 0-5    | 5      | 0-6    | 3      | 0-7    | 1     \n",
      "h_out | 0-1    | 5      | 0-4    | 2      | 0-5    | 5      | 0-6    | 3      | 0-7    | 1     \n",
      "\n",
      "Tokens 40 to 49:\n",
      "Layer | Token 40 | Token 41 | Token 42 | Token 43 | Token 44 | Token 45 | Token 46 | Token 47 | Token 48 | Token 49\n",
      "---------------------------------------------------------------------------------------\n",
      "h0_out | 0-2    | 0      | 0-2    | 1      | 0-4    | 1      | 0-5    | 0      | 0-7    | 0     \n",
      "h1_out | 0-1    | 0      | 0-1    | 1      | 0-2    | 0      | 0-7    | 0      | 0-4    | 0     \n",
      "h2_out | 0-1    | 3      | 0-1    | 3      | 0-9    | 3      | 0-2    | 3      | 0-6    | 3     \n",
      "h3_out | 0-8    | 2      | 0-9    | 3      | 0-3    | 8      | 0-2    | 0      | 0-5    | 4     \n",
      "h_out | 0-8    | 2      | 0-9    | 3      | 0-3    | 8      | 0-2    | 0      | 0-5    | 4     \n",
      "\n",
      "Tokens 50 to 59:\n",
      "Layer | Token 50 | Token 51 | Token 52 | Token 53 | Token 54 | Token 55 | Token 56 | Token 57 | Token 58 | Token 59\n",
      "---------------------------------------------------------------------------------------\n",
      "h0_out | 0-4    | 0      | 0-4    | 4      | 0-2    | 0      | 0-2    | 4      | 0-2    | 1     \n",
      "h1_out | 0-7    | 0      | 0-9    | 0      | 0-9    | 2      | 0-2    | 0      | 0-3    | 0     \n",
      "h2_out | 0-4    | 4      | 0-9    | 4      | 0-9    | 3      | 0-2    | 3      | 0-4    | 3     \n",
      "h3_out | 0-6    | 1      | 0-2    | 8      | 0-8    | 8      | 0-9    | 1      | 0-3    | 4     \n",
      "h_out | 0-6    | 1      | 0-2    | 8      | 0-8    | 8      | 0-9    | 1      | 0-3    | 4     \n",
      "\n",
      "Tokens 60 to 64:\n",
      "Layer | Token 60 | Token 61 | Token 62 | Token 63 | Token 64\n",
      "-----------------------------------------------\n",
      "h0_out | 0-5    | 1      | 0-8    | 1      | 0-8   \n",
      "h1_out | 0-9    | 0      | 0-9    | 2      | 0-9   \n",
      "h2_out | 0-5    | 4      | 0-3    | 4      | 0-3   \n",
      "h3_out | 0-3    | 1      | 0-6    | 7      | 0-3   \n",
      "h_out | 0-3    | 1      | 0-6    | 7      | 0-3   \n"
     ]
    }
   ],
   "source": [
    "model.print_top_logit_progression(train_set[50],2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d82929f0b2a0bc732f0b4a7de66959930da3979fa54e053e3cc98d8b0f7a869f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
