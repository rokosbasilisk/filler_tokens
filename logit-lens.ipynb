{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loki/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from src.match3 import *\n",
    "from src.utils import InputEmbedCausalTransformer\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import Levenshtein\n",
    "import json\n",
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/home/loki/projects/filler_tokens/output_dir/2024-08-14-22-matchdata-checkpoint-final/model_weights.pt\"\n",
    "CONFIG_FILE = \"/home/loki/projects/filler_tokens/misc/llama_d384l4h6.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/matchdata_trainset_2024-08-13.csv', header=None, names=[\"text\"])\n",
    "test_df = pd.read_csv('data/matchdata_testset_2024-08-13.csv', header=None, names=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = Match3VectorDataset(train_df, 3, 10, 10, 'P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = Match3VectorDataset(test_df, 3, 10, 10, 'P')\n",
    "print(test_set.input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_input_tensors(input_string):\n",
    "    return {\"input_ids\":test_set.tensorize_inputs_worker({\"text\":pd.Series([input_string], index=['text'], name='1999')}).squeeze()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_input_tensors(\"1 P A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_label_tensors(input_string):\n",
    "    return {\"labels\":test_set.tensorize_labels_worker({\"text\":pd.Series([input_string], index=['text'], name='1999')})}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_input_tensors(\"100 P A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockOutputWrapper(torch.nn.Module):\n",
    "    def __init__(self, block, unembed_matrix, norm):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.unembed_matrix = unembed_matrix\n",
    "        self.norm = norm\n",
    "        self.block_output_unembedded = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.block(*args, **kwargs)\n",
    "        if isinstance(output, tuple):\n",
    "            self.block_output_unembedded = self.unembed_matrix(self.norm(output[0]))\n",
    "            return output\n",
    "        else:\n",
    "            self.block_output_unembedded = self.unembed_matrix(self.norm(output))\n",
    "            return output\n",
    "\n",
    "    def reset_block_output(self):\n",
    "        self.block_output_unembedded = None\n",
    "\n",
    "class LlamaHelper:\n",
    "    def __init__(self, config_file, model_path, test_set):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        config = AutoConfig.from_pretrained(config_file)\n",
    "        model = InputEmbedCausalTransformer(AutoModelForCausalLM.from_config(config), test_set.input_dim)\n",
    "        state_dict = torch.load(model_path)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        model = model.to(self.device)\n",
    "        self.model = model\n",
    "        self.word_index_map = test_set.word_index_map\n",
    "        self.data_len = test_set.data_len\n",
    "        self.mod = test_set.mod\n",
    "        self.input_dim = test_set.input_dim\n",
    "        \n",
    "        for i, layer in enumerate(self.model.base_model.model.layers):\n",
    "            self.model.base_model.model.layers[i] = BlockOutputWrapper(layer, self.model.base_model.lm_head, self.model.base_model.model.norm)\n",
    "\n",
    "    def decode_tensors(self, sequence):\n",
    "        decoded_sequence = []\n",
    "        marker_found = False\n",
    "\n",
    "        for token in sequence:\n",
    "            token = token.item()\n",
    "            if token == -100:\n",
    "                if not marker_found:\n",
    "                    decoded_sequence.append(\"[MASK]\")\n",
    "                continue\n",
    "            elif token == 0:\n",
    "                decoded_sequence.append(\"[EOS]\")\n",
    "                break  # Stop decoding after EOS\n",
    "            elif token < len(self.word_index_map):\n",
    "                # Regular word\n",
    "                word = list(self.word_index_map.keys())[list(self.word_index_map.values()).index(token)]\n",
    "                decoded_sequence.append(word)\n",
    "                if word in [\"A\", \"P\"]:\n",
    "                    marker_found = True\n",
    "            else:\n",
    "                # Handle digit labels\n",
    "                offset = len(self.word_index_map)\n",
    "                if token < offset + self.data_len * 2:\n",
    "                    # Tuple index encoding\n",
    "                    idx = (token - offset) % self.data_len\n",
    "                    tuple_pos = (token - offset) // self.data_len\n",
    "                    decoded_sequence.append(f\"{tuple_pos}-{idx}\")\n",
    "                else:\n",
    "                    # Single digit or digit in tuple\n",
    "                    char_pos = (token - offset - self.data_len * 2) // self.mod\n",
    "                    digit = (token - offset - self.data_len * 2) % self.mod\n",
    "                    if char_pos == 0 or len(decoded_sequence) == 0 or not decoded_sequence[-1][-1].isdigit():\n",
    "                        decoded_sequence.append(str(digit))\n",
    "                    else:\n",
    "                        decoded_sequence[-1] += str(digit)\n",
    "\n",
    "        return \" \".join(decoded_sequence)\n",
    "\n",
    "\n",
    "    def set_add_attn_output(self, layer, add_output):\n",
    "        self.model.base_model.model.layers[layer].attn_add_tensor(add_output)\n",
    "\n",
    "    def get_attn_activations(self, layer):\n",
    "        return self.model.base_model.model.layers[layer].get_attn_activations()\n",
    "\n",
    "    def reset_all_layers(self):\n",
    "        for layer in self.model.base_model.model.layers:\n",
    "            layer.reset_block_output()\n",
    "            \n",
    "    @staticmethod\n",
    "    def get_tokens(model, layer_idx, input_ids, decode_tensors, num_layers, rank=1, device=\"cuda\", skip_idx=None, skip_random=False):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids.float().unsqueeze(0))\n",
    "            logits = outputs.logits\n",
    "        last_token_logits = logits[0, -1, :]\n",
    "        if layer_idx < num_layers:\n",
    "            layer = model.base_model.model.layers[layer_idx]\n",
    "            if layer.block_output_unembedded is not None:\n",
    "                last_token_logits = layer.block_output_unembedded[0, -1, :]\n",
    "\n",
    "        # Get top k values and indices, where k is max(rank + 10, 100) to allow for skipping and random selection\n",
    "        k = max(rank + 10, 100)\n",
    "        val, idx = torch.topk(last_token_logits, k)\n",
    "\n",
    "        # If skip_idx is provided, remove it from consideration\n",
    "        if skip_idx is not None:\n",
    "            mask = ~torch.isin(idx, torch.tensor(skip_idx, device=device))\n",
    "            idx = idx[mask]\n",
    "            val = val[mask]\n",
    "\n",
    "        if skip_random and skip_idx is not None:\n",
    "            # Randomly select from top 10 non-skipped tokens\n",
    "            random_idx = torch.randint(0, min(10, len(idx)), (1,))\n",
    "            selected_idx = random_idx.item()\n",
    "        else:\n",
    "            # Select the token at the specified rank (subtracting 1 because rank is 1-indexed)\n",
    "            selected_idx = min(rank - 1, len(idx) - 1)\n",
    "\n",
    "        token = decode_tensors(idx[selected_idx].unsqueeze(-1)).strip()\n",
    "        return token, idx[selected_idx].item()\n",
    "\n",
    "\n",
    "    def create_new_token_input(self, token_id):\n",
    "        new_input = torch.zeros(1, self.input_dim, dtype=torch.float16)\n",
    "        if token_id < len(self.word_index_map):\n",
    "            new_input[0, token_id] = 1\n",
    "        else:\n",
    "            # Handle digit sequences\n",
    "            offset = len(self.word_index_map)\n",
    "            if token_id < offset + self.data_len * 2:\n",
    "                # Tuple index encoding\n",
    "                idx = (token_id - offset) % self.data_len\n",
    "                tuple_pos = (token_id - offset) // self.data_len\n",
    "                new_input[0, offset + tuple_pos * self.data_len + idx] = 1\n",
    "            else:\n",
    "                # Single digit or digit in tuple\n",
    "                char_pos = (token_id - offset - self.data_len * 2) // self.mod\n",
    "                digit = (token_id - offset - self.data_len * 2) % self.mod\n",
    "                new_input[0, offset + self.data_len * 2 + char_pos * self.mod + digit] = 1\n",
    "        return new_input\n",
    "    \n",
    "\n",
    "    def print_logit_progression(self, inputs,\n",
    "                                max_new_tokens=len(test_set[0]['labels']),\n",
    "                                layer_number=None,\n",
    "                                rank=1,\n",
    "                                skip_idx=None,\n",
    "                                input_length=None,\n",
    "                                skip_random=False):\n",
    "\n",
    "        self.reset_all_layers()\n",
    "        num_layers = len(self.model.base_model.model.layers)\n",
    "        result_dict = {f\"h{i}_out\": [] for i in range(num_layers)}\n",
    "        result_dict[\"h_out\"] = []\n",
    "        input_ids = inputs['input_ids'].to(self.device)\n",
    "        if input_length:\n",
    "            input_ids = input_ids[:input_length]\n",
    "        generated_sequence = input_ids.clone()\n",
    "        for _ in range(max_new_tokens):\n",
    "            self.reset_all_layers()\n",
    "\n",
    "            if layer_number is not None:\n",
    "                if layer_number > num_layers:\n",
    "                    print(f\"Error: Layer number {layer_number} is out of range. Max layer is {num_layers}.\")\n",
    "                    return {}\n",
    "                token, token_id = self.get_tokens(self.model, layer_number, generated_sequence, self.decode_tensors, num_layers, rank, self.device, skip_idx, skip_random)\n",
    "                layer_name = f\"h{layer_number}_out\" if layer_number < num_layers else \"h_out\"\n",
    "                result_dict[layer_name].append(token)\n",
    "            else:\n",
    "                for i in range(num_layers + 1):\n",
    "                    token, token_id = self.get_tokens(self.model, i, generated_sequence, self.decode_tensors, num_layers, rank, self.device, skip_idx, skip_random)\n",
    "                    layer_name = f\"h{i}_out\" if i < num_layers else \"h_out\"\n",
    "                    result_dict[layer_name].append(token)\n",
    "\n",
    "            if token in [\"[EOS]\",\"True\",\"False\"]: break\n",
    "            try:\n",
    "                new_token_input = self.create_new_token_input(token_id).to(self.device)\n",
    "\n",
    "                generated_sequence = torch.cat([generated_sequence, new_token_input], dim=0)\n",
    "            except:\n",
    "                generated_sequence = torch.cat([generated_sequence], dim=0)\n",
    "                break\n",
    "        # Print results\n",
    "        if layer_number is not None:\n",
    "            layer_name = f\"h{layer_number}_out\" if layer_number < num_layers else \"h_out\"\n",
    "            print(f\"{rank}th highest logit for {layer_name}:\")\n",
    "            print(\" \".join(result_dict[layer_name]))\n",
    "        else:\n",
    "            print(f\"{rank}th highest logit:\")\n",
    "            for layer_name, tokens in result_dict.items():\n",
    "                print(f\"{layer_name:<5}: \" + \" \".join(tokens))\n",
    "        return result_dict\n",
    "\n",
    "    def get_layer_logits(self, inputs, layer_idx):\n",
    "        self.reset_all_layers()\n",
    "        num_layers = len(self.model.base_model.model.layers)\n",
    "        \n",
    "        if layer_idx > num_layers:\n",
    "            raise ValueError(f\"Error: Layer number {layer_idx} is out of range. Max layer is {num_layers}.\")\n",
    "        \n",
    "        input_ids = inputs['input_ids'].to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids.float().unsqueeze(0))\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        if layer_idx < num_layers:\n",
    "            layer = self.model.base_model.model.layers[layer_idx]\n",
    "            if layer.block_output_unembedded is not None:\n",
    "                logits = layer.block_output_unembedded\n",
    "        \n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaHelper(CONFIG_FILE, MODEL_PATH, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "\n",
    "def run_print_logit_progression(model, test_set_item, rank, layer_number=None, skip_idx=None, skip_random=False):\n",
    "    return model.print_logit_progression(\n",
    "        test_set_item, \n",
    "        rank=rank, \n",
    "        layer_number=layer_number, \n",
    "        skip_idx=skip_idx, \n",
    "        input_length=11, \n",
    "        skip_random=skip_random\n",
    "    )\n",
    "\n",
    "results_list = []\n",
    "for rank in [1, 2, 3]:\n",
    "    results_df = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        for idx in tqdm(range(len(test_df))):\n",
    "            futures = [\n",
    "                executor.submit(run_print_logit_progression, model, test_set[idx], rank),\n",
    "                executor.submit(run_print_logit_progression, model, test_set[idx], rank, 4, 3),\n",
    "                executor.submit(run_print_logit_progression, model, test_set[idx], rank, 4, 3, True)\n",
    "            ]\n",
    "            \n",
    "            result, result_skip, result_skip_random = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "            \n",
    "            # Combine results\n",
    "            for k, v in result_skip.items():\n",
    "                result[k + \"_skip\"] = v\n",
    "            \n",
    "            for k, v in result_skip_random.items():\n",
    "                result[k + \"_skip_random\"] = v\n",
    "            \n",
    "            results_df.append(result)\n",
    "    \n",
    "    results_list.append({\n",
    "        'rank': rank,\n",
    "        'results': results_df\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(results_list, \"results_list.pkl\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d82929f0b2a0bc732f0b4a7de66959930da3979fa54e053e3cc98d8b0f7a869f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
