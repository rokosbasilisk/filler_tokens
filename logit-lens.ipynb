{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo:\n",
    "# - understand how the training data text is represented for chain of thought ot 3SUM sequences\n",
    "# - function for evaluating accuracy\n",
    "# - understand decoder implementation\n",
    "# - function to print rank-n for layer m and then iterate over using this\n",
    "\n",
    "\n",
    "# experiments:\n",
    "# - write the claims of the paper: COT is uninterpretable as it can be hidden, but this hidden COT still increases performs\n",
    "# - see where it is being replaced with '.' and where it is just the numbers\n",
    "# - check this across layer, across rank\n",
    "# - is this gradual or not gradual?\n",
    "# - check if these decoded sequences from different ranks and layers are in anyway simialr to the actual trianing dataset.\n",
    "# - write the punchline: you only see the decoded text of COT, but not what is there in the rest of the logits as information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/loki/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from data_utils import *\n",
    "from nethook import *\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from utils import InputEmbedCausalTransformer\n",
    "import json\n",
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"/home/loki/projects/filler_tokens/checkpoints/2024-07-21-17-minidata-checkpoint-final/model_weights.pt\"\n",
    "CONFIG_FILE = \"/home/loki/projects/filler_tokens/configs/llama_d384l4h6.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/match3data_trainset_2024-08-10.csv', header=None, names=[\"text\"])\n",
    "test_df = pd.read_csv('data/match3data_testset_2024-08-10.csv', header=None, names=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 072 673 359 052 321 177 612 692 768 730 P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A True'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 441 789 625 581 232 698 085 850 605 779 P 0- 1 0- 6 0- 9 0- 3 0- 9 6- 2 0- 2 8- 4 0- 1 2- 4 3- 6 4- 1 5- 3 1- 6 7- 9 8- 4 9- 4 3- 6 2- 7 5- 2 6- 0 2- 5 8- 2 9- 9 3- 7 5- 1 6- 5 7- 9- 3- 8 9- 0 4- 8 4- 7 4- 0 4- 8 9- 9 6- 3 7- 4 5- 9 5- 7 6- 8 6- 0 6- 5 8- 5 7- 5 8- 7 9- 7 A True'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validate encodings\n",
      "raw input 0  655 748 451 416 889 352 187 278 880 014 P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A False\n",
      "encoded sample 0 {'input_ids': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float16), 'labels': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
      "           3,    3,    4,    2,    0, -100])}\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "test_set = Match3VectorDataset(test_df, 3, 10, 10, 'P')\n",
    "print(test_set.input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockOutputWrapper(torch.nn.Module):\n",
    "    def __init__(self, block, unembed_matrix, norm):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "        self.unembed_matrix = unembed_matrix\n",
    "        self.norm = norm\n",
    "        self.block_output_unembedded = None\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.block(*args, **kwargs)\n",
    "        if isinstance(output, tuple):\n",
    "            self.block_output_unembedded = self.unembed_matrix(self.norm(output[0]))\n",
    "            return output\n",
    "        else:\n",
    "            self.block_output_unembedded = self.unembed_matrix(self.norm(output))\n",
    "            return output\n",
    "\n",
    "    def reset_block_output(self):\n",
    "        self.block_output_unembedded = None\n",
    "\n",
    "class LlamaHelper:\n",
    "    def __init__(self, config_file, model_path, test_set):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        config = AutoConfig.from_pretrained(config_file)\n",
    "        model = InputEmbedCausalTransformer(AutoModelForCausalLM.from_config(config), test_set.input_dim)\n",
    "        state_dict = torch.load(model_path)\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        model = model.to(self.device)\n",
    "        self.model = model\n",
    "        self.word_index_map = test_set.word_index_map\n",
    "        self.data_len = test_set.data_len\n",
    "        self.mod = test_set.mod\n",
    "        \n",
    "        for i, layer in enumerate(self.model.base_model.model.layers):\n",
    "            self.model.base_model.model.layers[i] = BlockOutputWrapper(layer, self.model.base_model.lm_head, self.model.base_model.model.norm)\n",
    "\n",
    "    def decode_tensors(self, sequence):\n",
    "        decoded_sequence = []\n",
    "        marker_found = False\n",
    "\n",
    "        for token in sequence:\n",
    "            token = token.item()\n",
    "            if token == -100:\n",
    "                if not marker_found:\n",
    "                    decoded_sequence.append(\"[MASK]\")\n",
    "                continue\n",
    "            elif token == 0:\n",
    "                decoded_sequence.append(\"[EOS]\")\n",
    "                break  # Stop decoding after EOS\n",
    "            elif token < len(self.word_index_map):\n",
    "                # Regular word\n",
    "                word = list(self.word_index_map.keys())[list(self.word_index_map.values()).index(token)]\n",
    "                decoded_sequence.append(word)\n",
    "                if word in [\"A\", \"P\"]:\n",
    "                    marker_found = True\n",
    "            else:\n",
    "                # Handle digit labels\n",
    "                offset = len(self.word_index_map)\n",
    "                if token < offset + self.data_len * 2:\n",
    "                    # Tuple index encoding\n",
    "                    idx = (token - offset) % self.data_len\n",
    "                    tuple_pos = (token - offset) // self.data_len\n",
    "                    decoded_sequence.append(f\"{tuple_pos}-{idx}\")\n",
    "                else:\n",
    "                    # Single digit or digit in tuple\n",
    "                    char_pos = (token - offset - self.data_len * 2) // self.mod\n",
    "                    digit = (token - offset - self.data_len * 2) % self.mod\n",
    "                    if char_pos == 0 or len(decoded_sequence) == 0 or not decoded_sequence[-1][-1].isdigit():\n",
    "                        decoded_sequence.append(str(digit))\n",
    "                    else:\n",
    "                        decoded_sequence[-1] += str(digit)\n",
    "\n",
    "        return \" \".join(decoded_sequence)\n",
    "\n",
    "\n",
    "    def get_logits(self, inputs):\n",
    "        with torch.no_grad():\n",
    "            input_ids = inputs['input_ids'].float().to(self.device).unsqueeze(0)\n",
    "            out = self.model(input_ids)\n",
    "            return out.logits\n",
    "\n",
    "    def set_add_attn_output(self, layer, add_output):\n",
    "        self.model.base_model.model.layers[layer].attn_add_tensor(add_output)\n",
    "\n",
    "    def get_attn_activations(self, layer):\n",
    "        return self.model.base_model.model.layers[layer].get_attn_activations()\n",
    "\n",
    "    def reset_all_layers(self):\n",
    "        for layer in self.model.base_model.model.layers:\n",
    "            layer.reset_block_output()\n",
    "\n",
    "    def print_logit_progression(self, inputs, layer_number=None, rank=1, sequence_length=None):\n",
    "        self.reset_all_layers()\n",
    "        logits = self.get_logits(inputs)\n",
    "\n",
    "        num_layers = len(self.model.base_model.model.layers)\n",
    "        seq_length = inputs['input_ids'].size(1) if sequence_length is None else min(sequence_length, inputs['input_ids'].size(1))\n",
    "\n",
    "        def process_layer(layer_idx):\n",
    "            tokens = []\n",
    "            values = []\n",
    "            layer = self.model.base_model.model.layers[layer_idx] if layer_idx < num_layers else None\n",
    "            for j in range(seq_length):\n",
    "                if layer and layer.block_output_unembedded is not None:\n",
    "                    layer_logits = layer.block_output_unembedded[0][j]\n",
    "                elif layer_idx == num_layers:  # Final output\n",
    "                    layer_logits = logits[0][j]\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                val, idx = torch.topk(layer_logits, rank)\n",
    "                tokens.append(self.decode_tensors(idx[-1].unsqueeze(-1)).strip())\n",
    "                values.append(val[-1].item())\n",
    "\n",
    "            return tokens, values\n",
    "\n",
    "        if layer_number is not None:\n",
    "            if layer_number > num_layers:\n",
    "                print(f\"Error: Layer number {layer_number} is out of range. Max layer is {num_layers}.\")\n",
    "                return\n",
    "\n",
    "            tokens, values = process_layer(layer_number)\n",
    "            layer_name = f\"h{layer_number}_out\" if layer_number < num_layers else \"h_out\"\n",
    "            print(f\"{rank}th highest logit progression for {layer_name}:\")\n",
    "            print(\"\".join(tokens))\n",
    "            print(\" \".join([f\"{v:.2f}\" for v in values]))\n",
    "        else:\n",
    "            print(f\"{rank}th highest logit progression:\")\n",
    "            for i in range(num_layers + 1):\n",
    "                tokens, values = process_layer(i)\n",
    "                layer_name = f\"h{i}_out\" if i < num_layers else \"h_out\"\n",
    "                print(f\"{layer_name:<5}: \" + \"\".join(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaHelper(CONFIG_FILE, MODEL_PATH, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 655 748 451 416 889 352 187 278 880 014 P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A False'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.iloc[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1th highest logit progression:\n",
      "h0_out: 0-10-80-860-0..0-00-00-0.......................................................\n",
      "h1_out: .................................................................\n",
      "h2_out: .................................................................\n",
      "h3_out: .................................................................\n",
      "h_out: .................................................................\n",
      "2th highest logit progression:\n",
      "h0_out: 0-060-0160-00-0...0-1AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "h1_out: 0-10-00-00-00-00-00-00-00-00-00-1AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "h2_out: 0-00-00-00-00-00-00-00-00-00-00-1AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "h3_out: 0-00-00-00-00-00-00-00-00-00-00-0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "h_out: 0-00-00-00-00-00-00-00-00-00-00-0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "\n",
      "\n",
      "1th highest logit progression:\n",
      "h0_out: .0-80-00-0..0-00-00-00-00-140-240-240-240-240-240-740-840-940-540-440-440-540-640-400-840-940-440-400-440-740-700-740-940-740-440-740-7\n",
      "h1_out: ..........0-140-240-240-840-840-540-740-840-940-240-340-440-540-140-140-840-940-240-440-940-640-740-840-940-740-940-640-7\n",
      "h2_out: ...........30-030-040-040-030-030-040-000-040-140-130-140-140-140-730-840-930-340-440-440-630-730-830-930-340-440-630-7\n",
      "h3_out: ...........90-010-000-090-090-050-710-870-940-110-380-120-550-630-720-180-170-240-440-580-240-200-280-920-340-390-310-3\n",
      "h_out: ...........90-010-000-090-090-050-710-870-940-110-380-120-550-630-720-180-170-240-440-580-240-200-280-920-340-390-310-3\n",
      "2th highest logit progression:\n",
      "h0_out: 0-060-8.0-00-00-80-8.6.00-500-500-800-800-500-870-200-000-200-810-510-800-810-820-900-210-710-720-210-620-840-880-710-910-910-920-8\n",
      "h1_out: 0-00-00-00-00-00-00-00-00-00-0.00-500-300-210-210-820-010-500-230-300-100-120-100-630-720-100-110-900-700-400-920-200-920-220-200-400-920-8\n",
      "h2_out: 0-00-00-00-00-00-00-00-00-00-00-100-240-330-330-540-610-730-830-930-230-340-330-530-630-140-130-140-230-230-530-240-240-940-240-430-330-340-3\n",
      "h3_out: 0-00-00-00-00-00-00-00-00-00-00-120-230-350-440-510-680-090-000-030-290-150-480-180-110-180-820-920-360-200-240-620-710-820-280-490-510-690-7\n",
      "h_out: 0-00-00-00-00-00-00-00-00-00-00-120-230-350-440-510-680-090-000-030-290-150-480-180-110-180-820-920-360-200-240-620-710-820-280-490-510-690-7\n",
      "\n",
      "\n",
      "1th highest logit progression:\n",
      "h0_out: ..0-80-0....0-010-140-540-240-440-540-600-800-840-910-440-540-510-840-740-800-840-940-740-740-800-800-800-800-240-910-410-510-4\n",
      "h1_out: ..........0-140-240-240-440-540-640-800-840-940-140-340-840-540-140-100-840-940-240-940-940-940-940-940-940-740-940-940-4\n",
      "h2_out: ...........00-040-030-030-030-030-030-840-030-140-140-140-140-140-730-840-930-340-440-540-630-730-940-940-340-430-630-7\n",
      "h3_out: ...........40-250-350-470-510-030-780-850-070-140-350-400-550-630-780-150-970-200-440-260-600-740-210-270-440-390-310-3\n",
      "h_out: ...........40-250-350-470-510-030-780-850-070-140-350-400-550-630-780-150-970-200-440-260-600-740-210-270-440-390-310-3\n",
      "2th highest logit progression:\n",
      "h0_out: 0-10-80-50-80-00-00-00-06..00-200-510-810-800-740-210-200-240-110-710-740-700-610-710-200-210-500-510-540-740-240-440-810-740-940-440-5\n",
      "h1_out: 0-00-00-00-00-00-00-00-00-00-0.00-500-300-010-800-800-740-000-030-330-110-100-800-600-840-100-110-900-790-500-700-700-800-890-900-400-400-7\n",
      "h2_out: 0-00-00-00-00-00-00-00-00-00-00-110-230-310-410-540-640-700-000-940-230-330-330-530-630-140-130-140-230-230-630-740-940-830-730-430-340-340-3\n",
      "h3_out: 0-00-00-00-00-00-00-00-00-00-00-150-040-080-060-050-610-000-010-940-230-130-180-160-110-110-810-190-340-280-540-210-260-840-910-390-530-630-7\n",
      "h_out: 0-00-00-00-00-00-00-00-00-00-00-150-040-080-060-050-610-000-010-940-230-130-180-160-110-110-810-190-340-280-540-210-260-840-910-390-530-630-7\n",
      "\n",
      "\n",
      "1th highest logit progression:\n",
      "h0_out: 0-0.0-8.0-00-00-2.0-0..40-200-840-240-540-840-700-800-940-440-440-440-540-840-700-840-240-740-740-840-700-700-840-910-440-410-440-7\n",
      "h1_out: ...........40-240-340-240-540-840-700-840-940-140-140-440-540-140-740-840-940-240-340-540-640-740-840-940-240-440-940-7\n",
      "h2_out: ...........30-030-040-030-040-030-030-840-040-140-130-140-140-130-730-840-140-340-440-540-630-730-840-940-340-440-330-7\n",
      "h3_out: ...........90-070-390-000-510-080-080-840-040-240-330-190-540-180-780-140-190-240-280-280-630-720-840-270-440-390-330-3\n",
      "h_out: ...........90-070-390-000-510-080-080-840-040-240-330-190-540-180-780-140-190-240-280-280-630-720-840-270-440-390-330-3\n",
      "2th highest logit progression:\n",
      "h0_out: .A0-00-01.0-80-0.0-00-100-540-410-800-800-200-820-240-010-110-710-810-810-250-810-270-810-500-510-700-940-810-200-740-710-740-710-8\n",
      "h1_out: 0-00-00-00-00-00-00-00-00-00-00-100-000-890-800-000-500-020-000-010-330-300-890-100-830-100-100-820-900-990-900-900-900-920-220-900-900-420-9\n",
      "h2_out: 0-00-00-00-00-00-00-00-00-00-00-100-210-330-300-530-610-700-000-930-230-340-330-530-640-140-130-930-230-230-230-240-940-230-230-430-330-640-3\n",
      "h3_out: 0-00-00-00-00-00-00-00-00-00-00-020-280-010-480-040-630-700-050-970-130-180-440-160-630-110-890-970-300-440-540-200-240-280-910-330-540-610-7\n",
      "h_out: 0-00-00-00-00-00-00-00-00-00-00-020-280-010-480-040-630-700-050-970-130-180-440-160-630-110-890-970-300-440-540-200-240-280-910-330-540-610-7\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    model.print_logit_progression(test_set[i], rank=1, sequence_length=len(test_set[i]['input_ids']))\n",
    "    model.print_logit_progression(test_set[i], rank=2, sequence_length=len(test_set[i]['input_ids']))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d82929f0b2a0bc732f0b4a7de66959930da3979fa54e053e3cc98d8b0f7a869f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
