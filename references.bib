@article{pfau2023let,
  title={Let's Think Dot by Dot: Hidden Computation in Transformer Language Models},
  author={Pfau, Jacob and Merrill, William and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2404.15758},
  year={2023},
  url={https://arxiv.org/abs/2404.15758}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Zhao, Ed and Zhu, Sifei and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022},
  url={https://arxiv.org/abs/2201.11903}
}

@misc{nostalgebraist2020interpreting,
  author={nostalgebraist},
  title={Interpreting {GPT}: the logit lens},
  year={2020},
  howpublished={\url{https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/}},
  note={Accessed: 2024-04-25}
}

@article{touvron2023llama,
  title={{LLaMA}: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023},
  url={https://arxiv.org/abs/2302.13971}
}
