@article{pfau2023let,
  title={Let's Think Dot by Dot: Hidden Computation in Transformer Language Models},
  author={Pfau, Jacob and Merrill, William and Bowman, Samuel R.},
  journal={arXiv preprint arXiv:2404.15758},
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{sanford2023representational,
  title={Representational strengths and limitations of transformers},
  author={Sanford, Clayton and Hsu, Daniel J and Telgarsky, Matus},
  journal={arXiv preprint arXiv:2306.02896},
  year={2023}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}
